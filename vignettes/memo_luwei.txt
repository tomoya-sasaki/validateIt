* extension for recordTasks
* How to use sendTasks
* When to use goldmix

* suggestion for and validateTopic/validateLabel recordTask --> store the data

* MTurk is easy to collect people $0.08/question
* One unique work per task
* Master workers: a little bit expensive than the average (20% increase), choose on the website
* 500 HITS (= 500 passages) for each model: you never know how many workers do the tasks until it is finished --> you can increase quality of hits
* 


* [Create project] Sentiment Analysis
	* number of assignment per task: 1
	* 1 hour time allocation
	* Task expires quick or slow? quicker is better: 7 hours
	* choose (1) master workers (2) qualification tasks --> 
* Reject HITS: if they miss gold standard HITS or spend too less time on each HIT



1. We would like to check our understanding of the workflow. 
a) Collect workers from MTurk: 
b) Qualification task: tailored for your own analysis and documents (automate the function to create)
	* Check qualification task: cannot check
	* Check using sandbox
c) Use MTurk qualification tasks to filter workers (using xml files)
d) Ask the filtered worker to complete the task with survey format generated by recordTasks() and sendTasks() (you have used SentimentIt here? Do we need to use labelR now?). We would also like to know if this step can be completed within the MTurk (i.e., not using an outside platform such as Qualtrics)
e) Extract the results using getResults()

layouts: WI and T8WSI.html
Choose gold standard: you just need to choose qualitatively (1/11 HITS, around 10% of different gold standard HITS among the total HITS)
IRB: tell "we do not study human, we compare models"